---
title: "Chapter 4"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```

### EXAMPLE 4 - 1(1) MULTIPLE LINEAR REGRESSION 

#### Input data
```{r}
x1 <- c(274, 180, 375, 205, 86, 265, 98, 330, 195, 53, 430, 372, 236, 157, 370)
x2 <- c(2450, 3254, 3802, 2838, 2347, 3782, 3008, 2450, 2137, 2560, 4020, 4427, 2660, 2088, 2605)
y <- c(162, 120, 223, 131, 67, 169, 81, 192, 116, 55, 252, 232, 144, 103, 212)

All <- data.frame(x1,x2,y)
```

#### Visualization
```{r}
library(psych)
pairs.panels(All, hist.col = "#00AFBB", density = TRUE, scale = TRUE,ellipses = FALSE, cex.cor = 0.7)
```

#### Discriptive statistics
```{r}
ds <- function(x) {
  c(sum(x),mean(x), sum(x^2),var(x), sd(x))
}

All_ds <- data.frame(rbind(ds(x1),ds(x2),ds(y)))
names(All_ds) <- c("SUM","MEAN","SS","VAR","SD")
row.names(All_ds) <- c("x1","x2","y")
print(All_ds)

# correlation
cor(All[,c("y","x1","x2")])
```

#### Model fitting
```{r}
# model crossproducts X'X, X'Y, Y'Y
intercept <- rep(1,15)
all_new <- as.matrix(cbind(intercept,All))
cross <- crossprod(all_new)

# linear regression
model <- lm(y ~ x1 + x2, data = All)
anova(model)
summary(model)

# confidence limits
confint(model, level = 0.95)

# covariance of estimates
library(MASS)
vcov(model)

```


### EXAMPLE 4 - 1(2) Model Prediction

```{r}
# new data
x1 <- c(x1, 220, 375)
x2 <- c(x2, 2500, 3500)
new_x <- data.frame(cbind(x1, x2))

# prediction
predict <- predict(model,newdata = new_x)
residual <- c(resid(model),NA,NA)

CL_mean <- as.data.frame(predict(model, newdata = new_x,interval = "confidence"))
LCLM <- CL_mean$lwr
UCLM <- CL_mean$upr
CL_predict <- as.data.frame(predict(model, newdata = new_x, interval = "prediction"))
LCL <- CL_predict$lwr
UCL <- CL_predict$upr

# present values
y <- c(y,rep(NA,2))
pred <- cbind(y, new_x, predict, LCLM, UCLM, LCL, UCL)
print(pred)

```


### EXAMPLE 4 - 2(1) Sequential Test and Partial Test

#### Data input
```{r}
rm(list=ls())
x1 <- c(1.31, 1.55, 0.99, 0.99, 1.05, 1.09, 1.08, 1.27, 0.99, 1.34, 1.3, 1.33, 1.86, 1.58, 1.97, 1.8, 1.75, 1.72, 1.68, 1.75, 2.19, 1.73)
x2 <- c(1.07, 1.49, 0.84, 0.83, 0.9, 0.93, 0.9, 1.08, 0.85, 1.13, 1.1, 1.1, 1.47, 1.34, 1.59, 1.56, 1.58, 1.43, 1.57, 1.59, 1.86, 1.67)
x3 <- c(0.44, 0.53, 0.34, 0.34, 0.36, 0.42, 0.4, 0.44, 0.36, 0.45, 0.45, 0.48, 0.6, 0.52, 0.67, 0.66, 0.63, 0.64, 0.72, 0.68, 0.75, 0.64)
x4 <- c(0.75, 0.9, 0.57, 0.54, 0.64, 0.61, 0.51, 0.77, 0.56, 0.77, 0.76, 0.77, 1.01, 0.95, 1.2, 1.02, 1.09, 1.02, 0.96, 1.08, 1.24, 1.14)
x5 <- c(0.35, 0.47, 0.32, 0.27, 0.3, 0.31, 0.31, 0.34, 0.29, 0.37, 0.38, 0.38, 0.65, 0.5, 0.59, 0.59, 0.59, 0.63, 0.68, 0.62, 0.72, 0.55)
y <- c(1.95, 2.9, 0.72, 0.81, 1.09, 1.22, 1.02, 1.93, 0.64, 2.08, 1.98, 1.9, 8.56, 4.49, 8.49, 6.17, 7.54, 6.36, 7.63, 7.78, 10.15, 6.88)

All <- data.frame(x1,x2,x3,x4,x5,y)
```

#### Discriptive statistics
```{r}
ds <- function(x) {
  c(sum(x),mean(x), sum(x^2),var(x), sd(x))
}

All_ds <- data.frame(rbind(ds(x1),ds(x2),ds(x3),ds(x4),ds(x5),ds(y)))
names(All_ds) <- c("SUM","MEAN","SS","VAR","SD")
row.names(All_ds) <- c("x1","x2","x3","x4","x5","y")
print(All_ds)

# correlation
round(cor(All[,c("y","x1","x2","x3","x4","x5")]),4)
```


#### Regression analysis
```{r}
model <- lm(y~.,data = All)

summary(model)
anova(model) # Type I tests

library(car)
Anova(model, type="II")   # Type II test
 
```


### EXAMPLE 4 - 2(2) Illustrate STB Estimation

#### Data input
```{r}
rm(list=ls())
x1 <- c(1.31, 1.55, 0.99, 0.99, 1.05, 1.09, 1.08, 1.27, 0.99, 1.34, 1.3, 1.33, 1.86, 1.58, 1.97, 1.8, 1.75, 1.72, 1.68, 1.75, 2.19, 1.73)
x2 <- c(1.07, 1.49, 0.84, 0.83, 0.9, 0.93, 0.9, 1.08, 0.85, 1.13, 1.1, 1.1, 1.47, 1.34, 1.59, 1.56, 1.58, 1.43, 1.57, 1.59, 1.86, 1.67)
x3 <- c(0.44, 0.53, 0.34, 0.34, 0.36, 0.42, 0.4, 0.44, 0.36, 0.45, 0.45, 0.48, 0.6, 0.52, 0.67, 0.66, 0.63, 0.64, 0.72, 0.68, 0.75, 0.64)
x4 <- c(0.75, 0.9, 0.57, 0.54, 0.64, 0.61, 0.51, 0.77, 0.56, 0.77, 0.76, 0.77, 1.01, 0.95, 1.2, 1.02, 1.09, 1.02, 0.96, 1.08, 1.24, 1.14)
x5 <- c(0.35, 0.47, 0.32, 0.27, 0.3, 0.31, 0.31, 0.34, 0.29, 0.37, 0.38, 0.38, 0.65, 0.5, 0.59, 0.59, 0.59, 0.63, 0.68, 0.62, 0.72, 0.55)
y <- c(1.95, 2.9, 0.72, 0.81, 1.09, 1.22, 1.02, 1.93, 0.64, 2.08, 1.98, 1.9, 8.56, 4.49, 8.49, 6.17, 7.54, 6.36, 7.63, 7.78, 10.15, 6.88)

xx1 <- x1*100
xxx1 <- x1/100
All1 <- data.frame(x1,x2,x3,x4,x5,y)
All2 <- data.frame(xx1,x2,x3,x4,x5,y)
All3 <- data.frame(xxx1,x2,x3,x4,x5,y)

```


#### OLS
```{r}
# regression 
model1 <- lm(y~.,data = All1)

summary(model1)

# generate the standardized parameter estimates
# install.packages("QuantPsyc") 
library(QuantPsyc)
lm.beta(model1)
```

#### x1 is multiplied by 100
```{r}
# regression 
model2 <- lm(y~.,data = All2)

summary(model2)

# generate the standardized parameter estimates
lm.beta(model2)
```

#### x1 is divided by 100
```{r}
# regression 
model3 <- lm(y~.,data = All3)

summary(model3)

# generate the standardized parameter estimates
lm.beta(model3)
```



### EXAMPLE 4 - 2(3) Hypothesis Test

#### Data input
```{r}
rm(list=ls())
x1 <- c(1.31, 1.55, 0.99, 0.99, 1.05, 1.09, 1.08, 1.27, 0.99, 1.34, 1.3, 1.33, 1.86, 1.58, 1.97, 1.8, 1.75, 1.72, 1.68, 1.75, 2.19, 1.73)
x2 <- c(1.07, 1.49, 0.84, 0.83, 0.9, 0.93, 0.9, 1.08, 0.85, 1.13, 1.1, 1.1, 1.47, 1.34, 1.59, 1.56, 1.58, 1.43, 1.57, 1.59, 1.86, 1.67)
x3 <- c(0.44, 0.53, 0.34, 0.34, 0.36, 0.42, 0.4, 0.44, 0.36, 0.45, 0.45, 0.48, 0.6, 0.52, 0.67, 0.66, 0.63, 0.64, 0.72, 0.68, 0.75, 0.64)
x4 <- c(0.75, 0.9, 0.57, 0.54, 0.64, 0.61, 0.51, 0.77, 0.56, 0.77, 0.76, 0.77, 1.01, 0.95, 1.2, 1.02, 1.09, 1.02, 0.96, 1.08, 1.24, 1.14)
x5 <- c(0.35, 0.47, 0.32, 0.27, 0.3, 0.31, 0.31, 0.34, 0.29, 0.37, 0.38, 0.38, 0.65, 0.5, 0.59, 0.59, 0.59, 0.63, 0.68, 0.62, 0.72, 0.55)
y <- c(1.95, 2.9, 0.72, 0.81, 1.09, 1.22, 1.02, 1.93, 0.64, 2.08, 1.98, 1.9, 8.56, 4.49, 8.49, 6.17, 7.54, 6.36, 7.63, 7.78, 10.15, 6.88)

All <- data.frame(x1,x2,x3,x4,x5,y)

```

#### General hypothesis test 
```{r}
# Regression
model <- lm(y ~., data = All)

summary(model)
# Test beta4=beta5=0
library(car)
linearHypothesis(model,c("x4","x5"),test = "F") # "test = F" means specifying whether to compute the finite-sample F statistic (with approximate F distribution) 
```










